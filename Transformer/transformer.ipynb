{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "# torch中变量封装函数Variable.\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9090, -1.7135,  0.9683,  0.7592,  1.4060],\n",
       "         [-0.1570,  0.4284,  1.4959, -0.0457, -0.2729],\n",
       "         [ 1.6651, -0.9901, -0.4175,  0.6405, -1.6994],\n",
       "         [ 1.0410, -0.2590,  0.4833, -0.3428, -0.0436]],\n",
       "\n",
       "        [[ 1.5341,  0.2422,  1.5804, -0.5379,  1.4425],\n",
       "         [ 0.0678,  0.1272,  0.2976, -0.3707, -1.2650],\n",
       "         [-1.0305, -1.4155,  1.1561, -0.0900,  1.3504],\n",
       "         [-2.2858,  0.2465,  0.2565, -0.9511, -1.1107]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = nn.Embedding(20, 5)\n",
    "input = torch.LongTensor([[1,3,16,7], [10,9,5,2]])\n",
    "emb(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000],\n",
       "         [-0.3544,  1.3096, -1.5186],\n",
       "         [-0.0525,  0.5067, -1.0424],\n",
       "         [ 0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.0766, -0.2262,  0.1055],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.7510,  0.4329, -0.4989],\n",
       "         [-0.9452,  0.2293, -0.4351]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = nn.Embedding(20, 3, padding_idx=0)\n",
    "input = torch.LongTensor([[0,3,16,0], [10,0,5,2]])\n",
    "emb(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        \"\"\"类的初始化函数，有两个參数，d_model：指词嵌入的维度，vocab：指词表的大小.\"\"\"\n",
    "        # 接着就是使用super的方式指明继承nn.Module的初始化函数，我们自己实现的所有层都会这样\n",
    "        super(Embeddings, self).__init__()\n",
    "        # 之后就是调用nn中的预定义层Embedding，获得一个词嵌入对象self.lut\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        # 最后就是将d_model传入类中\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"可以将其理解为该层的前向传播逻辑，所有层中都会有此函数\n",
    "        当传给该类的实例化对象參数时，自动调用该类函数\n",
    "        参数x：因为Embedding层是首层，所以代表输入给模型的文本通过词汇映射后的张量\"\"\"\n",
    "        # 将x传给self.lut井与根号下self.d_mode1相乘作为结果返回, 缩放\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义位置编码器类，我们同样把它看做一个层，因此会继承nn.Module 口\n",
    "class PositionalEncoding(nn. Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        \"\"\"位置编码器类的初始化函数，共有三个參数，分别是d_model:词嵌入维度，\n",
    "        dropout:置0比率，max_len:每个句子的最大长度\"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # 实例化nn中预定义的Dropout层，并将dropout传入其中，获得对象self.dropout\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # 初始化一个位置编码矩阵，它是一个0阵，矩阵的大小是max_len x d_model.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # 初始化一个绝对位置矩阵，在我们这里，词汇的绝对位置就是用它的索引去表示．\n",
    "        # 所以我们首先使用arange方法获得一个连续自然数向量，然后再使用unsqueeze方法拓展向量维度\n",
    "        # 又因为参数传的是1，代表矩阵拓展的位置，会使向量变成一个max-len x 1 的矩阵，\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        # 绝对位置矩阵初始化之后，接下来就是考虑如何将这些位置信息加入到位置编码矩阵中，\n",
    "        # 最简单思路就是先将max_1en x 1的绝对位置矩阵，变换成max_len x d_model形状，然后覆盖原来的初始位置编码矩阵即可，\n",
    "        # 要做这种矩阵变换，就需要一个1xd_model形状的变换矩阵div_term，我们对这个变换矩阵的要习除了形状外\n",
    "        # 还希望它能够将自然数的绝对位置编码缩放成足够小的数字，有助于在之后的梯度下降过程中更快的收敛，这样我们就可以开始初始\n",
    "        # 首先使用arange获得一个自然数矩阵，但是细心的同学们会发现，我们这里并没有按照预计的一样初始化一个1×d_model的矩阵\n",
    "        # 而是有了一个跳跃，只初始化了一半即1xd_model/2 的矩阵。为什么是一半呢，其实这里并不是真正意义上的初始化了一半的矩阵\n",
    "        # 我们可以把它看作是初始化了两次，而每次初始化的变换矩阵会做不同的处理，第一次初始化的变换矩阵分布在正玄波上，第二次\n",
    "        # 并把这两个矩阵分别填充在位置编码矩阵的偶数和奇数位置上，组成最终的位置编码矩阵．\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * \n",
    "                            -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # 这样我们就得到了位置编码矩阵pe，pe现在还只是一个二维矩阵，要想和embedding的输出\n",
    "        # 就必须拓展一个维度，所以这里使用unsqueeze拓展维度．\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # 最后把pe位置编码矩阵注册成模型的buffer，什么是buffer呢，\n",
    "        # 我们把它认为是对模型效果有帮助的，但是却不是模型结构中超參数或者參数，不需要随着优化步骤进行更新\n",
    "        # 注册之后我们就可以在模型保存后重加载时和模型结构与参数一同被加载．\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"forward函数的参数是x，表示文本序列的词嵌入表示\"\"\"\n",
    "        # 在相加之前我们对pe做一些适配工作， 将这个三维张量的第二维也就是句子最大长度的那一维将切\n",
    "        # 因为我们默认max-1en为5000一般来讲实在太大了，很难有一条句子包含5800个词汇，所以要进行\n",
    "        # 最后使用Variable进行封装，使其与x的样式相同，但是它是不需要进行梯度求解的，因此把requ\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n",
    "        # 最后使用self.dropout对象进行\"丢弃\"操作，并返回结果.\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1562, -0.6876,  0.0424,  0.0000,  1.2023],\n",
       "        [-3.2916, -1.2298,  0.3103, -0.0000, -2.2196],\n",
       "        [ 2.3992, -3.1739,  1.1517,  0.4525, -1.0792],\n",
       "        [ 0.6800, -2.0477,  0.4588,  0.7145,  0.0000]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Dropout(p=0.2)\n",
    "input =  torch.randn(4, 5)\n",
    "output = m(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4])\n",
    "torch.unsqueeze(x, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词嵌入维度是512维度\n",
    "d_model = 512\n",
    "\n",
    "vocab = 1000\n",
    "\n",
    "# 置0比率为0.1\n",
    "dropout = 0.1\n",
    "\n",
    "# 句子最大长度\n",
    "max_len = 60\n",
    "\n",
    "# 输入x是Embedding层的输出张量，形状是2×4×512\n",
    "x = Variable(torch.LongTensor([[100, 2, 421, 500], [491, 999, 1, 220]]))\n",
    "emb = Embeddings(d_model, vocab)\n",
    "embr = emb(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pe_result: tensor([[[ 37.3150,  -4.8123,  -1.1419,  ...,   8.3940, -16.7525,  45.9313],\n",
      "         [  0.0000,  11.2654,  27.7836,  ..., -17.2091, -28.4949,  -1.8657],\n",
      "         [-51.7508, -18.5505,   3.6940,  ..., -11.0372, -21.4976,  10.9296],\n",
      "         [ -6.9911,  14.9293, -36.5244,  ...,   0.0000,  -0.0000,  -6.6375]],\n",
      "\n",
      "        [[-36.4800, -45.4464,  -8.2772,  ...,  26.8458,  18.3403, -20.4179],\n",
      "         [-13.0741, -18.5222,  16.2227,  ...,  11.7240,   0.0000, -27.1928],\n",
      "         [ -0.0000, -11.4216, -27.2937,  ...,  -9.3008,   0.0000, -25.8826],\n",
      "         [  0.7426,  55.6398, -39.4706,  ..., -40.1964,  27.6500,  32.6713]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = embr\n",
    "pe = PositionalEncoding(d_model, dropout, max_len)\n",
    "pe_result = pe(x)\n",
    "print(\"pe_result:\", pe_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 第一步设置一个画布\n",
    "# plt.figure(figsize=(15, 5)）\n",
    "# # 实例代PositionalEncoding类对象，词嵌入维度给20，置零比率设置为0\n",
    "# pe = Positiona lEncoding(20, 0)\n",
    "# # 向pe中传入一个全零初始化的x，相当于展示pe\n",
    "# y = pe(Variable(torch.zeros(1, 100, 20))）\n",
    "# plt.plot(np.arange(100), y[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"\"\"生成向后道掩的掩码张量，參数size是掩码张量最后两个维度的大小，它的最后两维形成一个方阵\"\"\"\n",
    "    # 在函数中，首先定义掩码张量的形状\n",
    "    attn_shape = (1, size, size)\n",
    "    # 然后使用np.ones方法向这个形状中添加1元素，形成上三角阵，最后为了节约空间，\n",
    "    # 再使其中的数据类型变为无符号8位整形unit8\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    # 最后将numpy类型转化为torch中的tensor，内部做一个1 的操作，\n",
    "    # 在这个其实是做了一个三角阵的反转，subsequent_mask中的每个元素都会被1减，\n",
    "    # 如果是0，subsequent.mask中的该位置由0变成1\n",
    "    # 如果是1，subsequent.mask中的该位置由1变成0\n",
    "    return torch.from_numpy(1 - subsequent_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3],\n",
       "       [ 4,  5,  6],\n",
       "       [ 0,  8,  9],\n",
       "       [ 0,  0, 12]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.triu([[1,2,3], [4,5,6],[7,8,9],[10,11,12]], k=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [0, 5, 6],\n",
       "       [0, 0, 9],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.triu([[1,2,3], [4,5,6],[7,8,9],[10,11,12]], k=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 3],\n",
       "       [0, 0, 6],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.triu([[1,2,3], [4,5,6],[7,8,9],[10,11,12]], k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"注意力机制的实现, 输入分别是query, key, value, mask: 掩码张量,\n",
    "        dropout是nn.Dropout层的实例化对象, 默认为None\"\"\"\n",
    "    # 在函数中，首先取query的最后一维的大小，一般情况下就等同于我们的词嵌入维度，命名为d_k\n",
    "    d_k = query.size(-1)\n",
    "    # 按照注意力公式，将query与key的转置相乘，这里面key是将最后两个维度进行转置，再除以缩放系数\n",
    "    # 得到注意力得分张量scores\n",
    "    scores = torch. matmul(query, key.transpose(-2,-1)) / math.sqrt (d_k)\n",
    "    # 接着判断是否使用掩码张量\n",
    "    if mask is not None:\n",
    "    # 使用tensor的masked_fi11方法，将掩码张量和scores张量每个位置一一比较，如果掩码张量处理\n",
    "    # 则对应的scores张量用-1e9这个值来替换，如下演示\n",
    "        scores = scores.masked_fil1(mask == 0,-1e9)\n",
    "    # 对scores的最后一维进行softmax操作，使用F.softmax方法，第一个参数是softmax对象，第二个\n",
    "    # 这样获得最终的注意力张量\n",
    "    p_attn = F.softmax(scores, dim =-1)\n",
    "    #之后判断是否使用dropout进行隨机置e\n",
    "    if dropout is not None:\n",
    "    # 将p_attn传入dropout对象中进行'丢弃\"处理\n",
    "        p_attn = dropout(p_attn)\n",
    "    # 最后，根据公式将p-attn与value张量相乘获得最终的query注意力表示，同时返回注意力张量\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
