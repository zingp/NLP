{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è¯­è¨€æ¨¡å‹\n",
    "------\n",
    "## 1 ç®€ä»‹\n",
    "- pytorchå®ç°LSTMè®­ç»ƒè¯­è¨€æ¨¡å‹\n",
    "- ä½¿ç”¨çš„æ•°æ®é›†ï¼š\n",
    "    - bobsue.lm.train.txtï¼šè¯­è¨€æ¨¡å‹è®­ç»ƒæ•°æ®ï¼ˆLMTRAINï¼‰\n",
    "    - bobsue.lm.dev.txtï¼šè¯­è¨€æ¨¡å‹éªŒè¯æ•°æ®ï¼ˆLMDEVï¼‰\n",
    "    - bobsue.lm.test.txtï¼šè¯­è¨€æ¨¡å‹æµ‹è¯•æ•°æ®ï¼ˆLMTESTï¼‰\n",
    "    - bobsue.prevsent.train.tsvï¼šåŸºäºä¸Šæ–‡çš„è¯­è¨€æ¨¡å‹è®­ç»ƒæ•°æ®ï¼ˆPREVSENTTRAINï¼‰\n",
    "    - bobsue.prevsent.dev.tsvï¼šåŸºäºä¸Šæ–‡çš„ä¸Šæ–‡è¯­è¨€æ¨¡å‹éªŒè¯æ•°æ®ï¼ˆPREVSENTDEVï¼‰\n",
    "    - bobsue.prevsent.test.tsvï¼šåŸºäºä¸Šæ–‡çš„ä¸Šæ–‡è¯­è¨€æ¨¡å‹æµ‹è¯•æ•°æ®ï¼ˆPREVSENTTESTï¼‰\n",
    "    - bobsue.voc.txtï¼šè¯æ±‡è¡¨æ–‡ä»¶ï¼Œæ¯è¡Œæ˜¯ä¸€ä¸ªå•è¯\n",
    "    - lmæ–‡ä»¶ä¸­çš„æ¯ä¸€è¡Œéƒ½åŒ…å«ä¸€ä¸ªæ•…äº‹ä¸­çš„å¥å­ã€‚prevæ–‡ä»¶ä¸­çš„æ¯ä¸€è¡Œéƒ½åŒ…å«ä¸€æ•…äº‹ä¸­çš„ä¸€ä¸ªå¥å­ï¼Œtabï¼Œç„¶åæ˜¯æ•…äº‹ä¸­çš„ä¸‹ä¸€ä¸ªå¥å­ã€‚æ³¨æ„ï¼šprevsentæ–‡ä»¶ä¸­æ¯ä¸€è¡Œçš„ç¬¬äºŒä¸ªå­—æ®µä¸ç›¸åº”çš„lmæ–‡ä»¶ä¸­çš„å¯¹åº”è¡Œç›¸åŒã€‚( ä¹Ÿå°±æ˜¯è¯´ï¼šcut -f 2 bobsue.prevsent.x.tsvä¸bobsue.lm.x.txtç›¸åŒï¼‰å®Œæ•´çš„è¯æ±‡è¡¨åŒ…å«åœ¨æ–‡ä»¶bobsue.voc.txtä¸­ï¼Œæ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªå•è¯ã€‚åœ¨è¿™ä¸ªä»»åŠ¡ä¸­ä¸ä¼šå‡ºç°æœªçŸ¥å•è¯ã€‚\n",
    "- è¯„ä¼°\n",
    "    - æˆ‘ä»¬ä½¿ç”¨å•è¯é¢„æµ‹å‡†ç¡®ç‡ä½œä¸ºä¸»è¦è¯„ä¼°æŒ‡æ ‡è€Œéå›°æƒ‘åº¦(perplexity)ã€‚å› ä¸ºå½“ä½ è¯•å›¾æ¯”è¾ƒæŸäº›æŸå¤±å‡½æ•°æ—¶ï¼Œperplexityä¸å¤ªå¥½ç”¨ã€‚\n",
    "\n",
    "-----\n",
    "## 2 å…·ä½“å†…å®¹\n",
    "### ç”¨Log Lossè®­ç»ƒLSTMæ¨¡å‹\n",
    "- å®ç°ä¸€ä¸ªåŸºäºLSTMçš„è¯­è¨€æ¨¡å‹ã€‚å…·ä½“ä¸ºï¼š\n",
    "    - å¯¹æ¯ä¸ªå½“å‰çš„hidden stateåšä¸€ä¸ªçº¿æ€§å˜åŒ–å’Œsoftmaxå¤„ç†ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ã€‚\n",
    "    - ä½¿ç”¨Log Loss (Cross Entropy Loss)æ¥è®­ç»ƒè¯¥æ¨¡å‹ã€‚ä½¿ç”¨**EVALLM**çš„æ­¥éª¤æ¥è¯„ä¼°æ¨¡å‹ã€‚\n",
    "    - æ±‡æŠ¥æ¨¡å‹è®­ç»ƒç»“æœå’Œä»£ç ã€‚ä½ çš„å•è¯é¢„æµ‹å‡†ç¡®ç‡åº”è¯¥èƒ½å¤Ÿè¾¾åˆ°30%ä»¥ä¸Šã€‚\n",
    "- è¦æ±‚\n",
    "    - è‡³å°‘è®­ç»ƒ10ä¸ªepoch\n",
    "    - å¯ä»¥ä½¿ç”¨ä¸åŒçš„æ¨¡å‹å‚æ•°ã€‚å»ºè®®ä½¿ç”¨ä¸€å±‚LSTMï¼Œ200 hidden dimensionä½œä¸ºè¯å‘é‡å’ŒLSTM hidden stateçš„å¤§å°ã€‚\n",
    "    - æ¨¡å‹å‚æ•°çš„åˆå§‹å€¼å¯ä»¥éšæœºè®¾å®š\n",
    "    - è¾“å…¥å’Œè¾“å‡ºå±‚çš„word embeddingå‚æ•°å¯ä»¥ä¸ä¸€æ ·ï¼Œå½“ç„¶ä½ ä¹Ÿå¯ä»¥å°è¯•æŠŠä»–ä»¬è®¾ç½®æˆä¸€æ ·ã€‚\n",
    "    - ä½¿ç”¨Adamæˆ–è€…SGDç­‰optimizeræ¥ä¼˜åŒ–æ¨¡å‹ã€‚\n",
    "    - åœ¨æäº¤æŠ¥å‘Šçš„æ—¶å€™è¯·å°½å¯èƒ½è¯¦ç»†æè¿°ä½ çš„æ‰€æœ‰æ¨¡å‹å‚æ•°ã€‚\n",
    "----\n",
    "### é”™è¯¯åˆ†æ\n",
    "- è¯·åœ¨ä½ çš„ä»£ç ä¸­æ·»åŠ ä¸€é¡¹åŠŸèƒ½ï¼Œå¯ä»¥å±•ç¤ºå‡ºä½ æ¨¡å‹é¢„æµ‹é”™è¯¯çš„å•è¯ï¼Œå°†æ ‡å‡†ç­”æ¡ˆå•è¯å’Œæ¨¡å‹é¢„æµ‹çš„å•è¯åˆ†åˆ«æ‰“å°å‡ºæ¥ã€‚\n",
    "- è¯·å†™ä¸‹ä½ çš„æ¨¡å‹æœ€å¸¸è§çš„35ä¸ªé¢„æµ‹é”™è¯¯(æ­£ç¡®ç­”æ¡ˆæ˜¯aï¼Œæ¨¡å‹é¢„æµ‹äº†b)ã€‚\n",
    "- é€šè¿‡è§‚å¯Ÿè¿™äº›å¸¸è§çš„é”™è¯¯ï¼Œå°†é”™è¯¯åˆ†ç±»ã€‚ä½ ä¸éœ€è¦å°†æ¯ä¸ªé”™è¯¯éƒ½åˆ†ç±»ï¼Œä¸è¿‡å»ºè®®åŒå­¦ä»¬èŠ±ç‚¹æ—¶é—´è§‚å¯Ÿè‡ªå·±æ¨¡å‹çš„é”™è¯¯ï¼Œçœ‹çœ‹ä»–ä»¬æ˜¯å¦æœ‰ä¸€å®šçš„ç›¸å…³æ€§ã€‚å¤§å®¶å¯ä»¥å°è¯•ä»ä»¥ä¸‹è§’åº¦æ€è€ƒé”™è¯¯ç±»å‹ï¼š\n",
    "    - ä¸ºä»€ä¹ˆä½ çš„æ¨¡å‹ä¼šé¢„æµ‹å‡ºè¿™ä¸ªå•è¯ï¼Ÿ\n",
    "    - æ¨¡å‹æ€ä¹ˆæ ·æ‰èƒ½åšå¾—æ›´å¥½ï¼Ÿè¿™ä¸ªæ¨¡å‹çŠ¯çš„é”™è¯¯æ˜¯å¾ˆæ¥è¿‘æ­£ç¡®ç­”æ¡ˆçš„å—ï¼Ÿå¦‚æœæ˜¯çš„è¯ï¼Œè¿™ä¸ªé”™è¯¯ç­”æ¡ˆä¸æ­£ç¡®ç­”æ¡ˆæœ‰ä½•ç›¸ä¼¼ä¹‹å¤„ï¼Ÿ\n",
    "    - æŠŠè¿™35ä¸ªé¢„æµ‹é”™è¯¯å½’ç±»æˆä½ å®šä¹‰çš„é”™è¯¯ç±»åˆ«ã€‚è®¨è®ºä¸€ä¸‹ä½ çš„æ¨¡å‹åœ¨å“ªäº›æ–¹é¢åšå¾—æ¯”è¾ƒå¥½ï¼Œå“ªäº›æ–¹é¢åšçš„ä¸å¥½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®æ–‡ä»¶\n",
    "word_file = './data/bobsue.voc.txt'\n",
    "train_file = './data/bobsue.lm.train.txt'\n",
    "test_file = './data/bobsue.lm.test.txt'\n",
    "dev_file = './data/bobsue.lm.dev.txt'\n",
    "\n",
    "BATCH_SIZE = 32       # æ‰¹æ¬¡å¤§å°\n",
    "EMBEDDING_DIM = 200   # è¯å‘é‡ç»´åº¦\n",
    "HIDDEN_DIM = 200      # éšå«å±‚\n",
    "GRAD_CLIP = 5.        # æ¢¯åº¦æˆªæ–­å€¼\n",
    "EPOCHS = 20 \n",
    "LEARN_RATE = 0.01     # åˆå§‹å­¦ä¹ ç‡\n",
    "\n",
    "BEST_VALID_LOSS = float('inf')     # åˆå§‹éªŒè¯é›†ä¸Šçš„æŸå¤±å€¼ï¼Œè®¾ä¸ºæœ€å¤§\n",
    "MODEL_PATH = \"lm-best-dim{}.pth\"   # æ¨¡å‹åç§°\n",
    "USE_CUDA = torch.cuda.is_available()    # æ˜¯å¦ä½¿ç”¨GPU\n",
    "NUM_CUDA = torch.cuda.device_count()    # GPUæ•°é‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 æ•°æ®é¢„å¤„ç†\n",
    "### 1.1 è¯»å–æ•°æ®æ–‡ä»¶ï¼Œæ„å»ºè¯æ±‡é›†ã€word2idxã€idx2wordã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_set(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        word_set = set([line.strip() for line in f])\n",
    "    return word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set = load_word_set(word_file)\n",
    "word2idx = {w:i for i, w in enumerate(word_set, 1)}\n",
    "idx2word = {i:w for i, w in enumerate(word_set, 1)}\n",
    "\n",
    "# å°†padçš„ç´¢å¼•è®¾ç½®ä¸º0å¹¶æ·»åŠ åˆ°è¯è¡¨\n",
    "PAD_IDX = 0\n",
    "word2idx[\"<pad>\"] = PAD_IDX\n",
    "idx2word[PAD_IDX] = \"<pad>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æ•°æ®å‡†å¤‡\n",
    "- å°†æ•°æ®å¤„ç†æˆæ¨¡å‹å¯ä»¥æ¥æ”¶çš„æ ¼å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(filename):\n",
    "    \"\"\"è¯»å–æ•°æ®é›†ï¼Œè¿”å›å¥å­åˆ—è¡¨\"\"\"\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        sentences = [line.strip() for line in f]\n",
    "    return sentences\n",
    "\n",
    "def sentences2words(sentences):\n",
    "    \"\"\"å°†å¥å­åˆ—è¡¨è½¬æ¢æˆå•è¯åˆ—è¡¨\"\"\"\n",
    "    return [w for s in sentences for w in s.split()]\n",
    "\n",
    "def max_sentence_num(sentences):\n",
    "    \"\"\"è¿”å›æœ€é•¿å¥å­å•è¯æ•°é‡\"\"\"\n",
    "    return max([len(s.split()) for s in sentences ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å„æ•°æ®é›†çš„å¥å­åˆ—è¡¨\n",
    "train_sentences = load_corpus(train_file)\n",
    "dev_sentences = load_corpus(dev_file)\n",
    "test_sentences = load_corpus(test_file)\n",
    "\n",
    "# å„æ•°æ®é›†çš„å•è¯åˆ—è¡¨\n",
    "train_words = sentences2words(train_sentences)\n",
    "dev_words = sentences2words(dev_sentences)\n",
    "test_words = sentences2words(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è®­ç»ƒé›†å¥å­æ•°: 6036, å•è¯æ•°: 71367.\n",
      "éªŒè¯é›†å¥å­æ•°: 750, å•è¯æ•°: 8707.\n",
      "æµ‹è¯•é›†å¥å­æ•°: 750, å•è¯æ•°: 8809.\n",
      "--------------------------------------------------\n",
      "è®­ç»ƒé›†æœ€é•¿å¥å­å•è¯ä¸ªæ•°ï¼š 21\n",
      "éªŒè¯é›†æœ€é•¿å¥å­å•è¯ä¸ªæ•°ï¼š 20\n",
      "æµ‹è¯•é›†æœ€é•¿å¥å­å•è¯ä¸ªæ•°ï¼š 21\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥çœ‹å¤„ç†åè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†çš„åŸºæœ¬æƒ…å†µ\n",
    "s = \"{}å¥å­æ•°: {}, å•è¯æ•°: {}.\"\n",
    "print(s.format(\"è®­ç»ƒé›†\", len(train_sentences), len(train_words)))\n",
    "print(s.format(\"éªŒè¯é›†\", len(dev_sentences), len(dev_words)))\n",
    "print(s.format(\"æµ‹è¯•é›†\", len(test_sentences), len(test_words)))\n",
    "\n",
    "print(\"-\"*50)\n",
    "\n",
    "# è¿™é‡Œéœ€è¦çŸ¥é“å„æ•°æ®é›†ä¸Šæœ€é•¿å¥å­çš„å•è¯ğŸ“šï¼Œä»¥ä¾¿åé¢æ„é€ å•è¯ç´¢å¼•å‘é‡çš„æ—¶å€™è®¾ç½®ä¸€ä¸ªæ°å½“çš„ç»´åº¦\n",
    "print(\"è®­ç»ƒé›†æœ€é•¿å¥å­å•è¯ä¸ªæ•°ï¼š\", max_sentence_num(train_sentences))\n",
    "print(\"éªŒè¯é›†æœ€é•¿å¥å­å•è¯ä¸ªæ•°ï¼š\", max_sentence_num(dev_sentences))\n",
    "print(\"æµ‹è¯•é›†æœ€é•¿å¥å­å•è¯ä¸ªæ•°ï¼š\", max_sentence_num(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_x_y(corpus, word2idx, seq_len=21):\n",
    "    \"\"\"\n",
    "    æ„é€ è¾“å…¥æ¨¡å‹çš„ç‰¹å¾ä»¥åŠæ ‡ç­¾ã€‚\n",
    "    è¾“å…¥ï¼š\n",
    "        corpusï¼š åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå¥å­ã€‚\n",
    "        word2idxï¼š å­—å…¸ï¼Œkeyæ˜¯å•è¯ï¼Œvalueæ˜¯å•è¯çš„ç´¢å¼•ã€‚\n",
    "        seq_lenï¼šint, å¥å­åˆ‡åˆ†åçš„å•è¯åºåˆ—çš„é•¿åº¦ã€‚\n",
    "    è¿”å›ï¼š\n",
    "        sentencesï¼šäºŒç»´åˆ—è¡¨ï¼Œæ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªå¥å­åˆ‡åˆ†åå•è¯çš„ç´¢å¼•åˆ—è¡¨ï¼ˆä¸åŒ…æ‹¬å¥å­çš„æœ€åä¸€ä¸ªå•è¯ï¼‰ã€‚è¾“å…¥æ¨¡å‹çš„xã€‚\n",
    "        labelsï¼šäºŒç»´åˆ—è¡¨ï¼Œæ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªå¥å­åˆ‡åˆ†åå•è¯çš„ç´¢å¼•åˆ—è¡¨ï¼ˆä¸åŒ…æ‹¬å¥å­çš„ç¬¬ä¸€ä¸ªå•è¯ï¼‰ã€‚yã€‚\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for sentence in corpus:\n",
    "        words = sentence.split()\n",
    "        sentence_vec = [0]*seq_len\n",
    "        for i, w in enumerate(words[:-1]):\n",
    "            sentence_vec[i] = word2idx[w]\n",
    "        sentences.append(sentence_vec)\n",
    "        \n",
    "        label_vec = [0] * seq_len\n",
    "        for i, w in enumerate(words[1:]):\n",
    "            label_vec[i] = word2idx[w]\n",
    "        labels.append(label_vec)\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_label = build_x_y(train_sentences, word2idx)\n",
    "dev_data, dev_label = build_x_y(dev_sentences, word2idx)\n",
    "test_data, test_label = build_x_y(test_sentences, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1075, 156, 471, 192, 460, 484, 943, 340, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "      [156, 471, 192, 460, 484, 943, 340, 534, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥çœ‹å¤„ç†åçš„è®­ç»ƒé›†ç¬¬ä¸€ä¸ªæ ·æœ¬åŠæ ‡ç­¾\n",
    "print(train_data[1])\n",
    "print(\" \"*5, train_label[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> The girl broke up with Bob . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "    The girl broke up with Bob . </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "print(\" \".join([idx2word[i] for i in train_data[idx]]))\n",
    "\n",
    "print(\" \"*3, \" \".join([idx2word[i] for i in train_label[idx]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ„é€ æ‰¹æ¬¡æ•°æ®\n",
    "def build_batch_data(data, label, batch_size=32):\n",
    "    \"\"\"æ„å»º batch tensorï¼Œè¿”å› batch åˆ—è¡¨ï¼Œæ¯ä¸ªbatchä¸ºäºŒå…ƒç»„åŒ…å«dataå’Œlabel\"\"\"\n",
    "    batch_data = []\n",
    "    data_tensor = torch.tensor(data, dtype=torch.long)\n",
    "    label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "    n, dim = data_tensor.size()\n",
    "    for start in range(0, n, batch_size):\n",
    "        end = start + batch_size\n",
    "        if end > n:\n",
    "            dbatch = data_tensor[start: ]\n",
    "            lbatch = label_tensor[start: ]\n",
    "            print(\"æœ€åä¸€ä¸ªbatch size:\", dbatch.size())\n",
    "            break\n",
    "        else:\n",
    "            dbatch = data_tensor[start: end]\n",
    "            lbatch = label_tensor[start: end]\n",
    "        batch_data.append((dbatch, lbatch))\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æœ€åä¸€ä¸ªbatch size: torch.Size([20, 21])\n",
      "æœ€åä¸€ä¸ªbatch size: torch.Size([14, 21])\n",
      "æœ€åä¸€ä¸ªbatch size: torch.Size([14, 21])\n"
     ]
    }
   ],
   "source": [
    "train_batch = build_batch_data(train_data, train_label, batch_size=BATCH_SIZE)\n",
    "dev_batch = build_batch_data(dev_data, dev_label, batch_size=BATCH_SIZE)\n",
    "test_batch = build_batch_data(test_data, test_label, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188 23 23\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥çœ‹å„æ•°æ®é›†æœ‰å¤šå°‘ä¸ªbatch\n",
    "print(len(train_batch), len(dev_batch), len(test_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 å®šä¹‰æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰æ¨¡å‹\n",
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(self.vocab_size, embedding_dim)\n",
    "        # batch_first=True æ„å‘³ç€è¾“å…¥æ˜¯(batch, seq, feature)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.hidden2word = nn.Linear(hidden_dim, self.vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeds = self.word_embeddings(x)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(embeds)\n",
    "        target_space = self.hidden2word(lstm_out.contiguous().view(-1, self.hidden_dim))\n",
    "        mask = (x != PAD_IDX).view(-1)\n",
    "        mask_target = target_space[mask]\n",
    "        \n",
    "        target_scores = F.log_softmax(mask_target, dim=1)\n",
    "        return target_scores\n",
    "\n",
    "    \n",
    "# è®¡ç®—å‡†ç¡®ç‡\n",
    "def acc_score(pred_score, y):\n",
    "    # è¿”å›æœ€å¤§çš„æ¦‚ç‡çš„ç´¢å¼•\n",
    "    y_pred = pred_score.argmax(dim=1)\n",
    "    # print(y.view(-1))\n",
    "    acc_count = torch.eq(y_pred, y.view(-1))\n",
    "    score = acc_count.sum().item() / acc_count.size()[0]\n",
    "    return score\n",
    "\n",
    "\n",
    "# è®­ç»ƒå‡½æ•°\n",
    "def train(model, device, iterator, optimizer, criterion, grad_clip):\n",
    "    epoch_loss = 0  # ç§¯ç´¯å˜é‡\n",
    "    epoch_acc = 0   # ç§¯ç´¯å˜é‡\n",
    "    model.train()   # è¯¥å‡½æ•°è¡¨ç¤ºPHASE=Train\n",
    "    \n",
    "    for x, y in iterator:  # æ‹¿æ¯ä¸€ä¸ªminibatch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        mask = y != PAD_IDX\n",
    "        pure_y = y[mask]\n",
    "        \n",
    "        fx = model(x)                 # è¿›è¡Œforward\n",
    "        loss = criterion(fx, pure_y)  # è®¡ç®—loss\n",
    "        acc = acc_score(fx, pure_y)   # è®¡ç®—å‡†ç¡®ç‡\n",
    "        loss.backward()               # è¿›è¡ŒBP\n",
    "        \n",
    "        # æ¢¯åº¦è£å‰ª\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()  # æ›´æ–°å‚æ•°\n",
    "        \n",
    "        epoch_loss += loss\n",
    "        epoch_acc += acc\n",
    "        \n",
    "    return epoch_loss/len(iterator),epoch_acc/len(iterator)\n",
    "\n",
    "\n",
    "# éªŒè¯å‡½æ•°ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†ç”¨ï¼Œä¸æ›´æ–°æ¢¯åº¦\n",
    "def evaluate(model, device, iterator, criterion):\n",
    "    model.eval()  # ä¸æ›´æ–°å‚æ•°ï¼Œé¢„æµ‹æ¨¡å¼\n",
    "    epoch_loss=0  # ç§¯ç´¯å˜é‡\n",
    "    epoch_acc=0   # ç§¯ç´¯å˜é‡\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in iterator:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            mask = y != PAD_IDX\n",
    "            pure_y = y[mask]\n",
    "            \n",
    "            fx = model(x)\n",
    "            loss = criterion(fx, pure_y)\n",
    "            acc = acc_score(fx, pure_y)\n",
    "            epoch_loss += loss\n",
    "            epoch_acc += acc\n",
    "    return epoch_loss/len(iterator), epoch_acc/len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 æ¨¡å‹è®­ç»ƒä¸è¯„ä»·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(word2idx)     # è¯æ±‡è¡¨é•¿åº¦\n",
    "\n",
    "model = MyLSTM(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else 'cpu')\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# ä½¿ç”¨å¤šå—GPU\n",
    "if NUM_CUDA > 1:\n",
    "    device_ids = list(range(NUM_CUDA))\n",
    "    print(device_ids)\n",
    "    model = nn.DataParallel(model, device_ids=device_ids)\n",
    "    # model = nn.parallel.DistributedDataParallel(model, device_ids=device_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ä¿å­˜æœ€ä¼˜æ¨¡å‹çš„é€»è¾‘æ˜¯ï¼Œæ¯ä¸€ä¸ªepochä¹‹åå†å¯¹æ¯”éªŒè¯é›†æŸå¤±å€¼ï¼ŒéªŒè¯é›†æŸå¤±é™ä½æ‰è®¤ä¸ºæ¨¡å‹æ›´ä¼˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1|Train Loss:3.447|Train Acc:0.32|Val Loss:3.509|Val Acc:0.3222\n",
      "Epoch:2|Train Loss:2.995|Train Acc:0.3483|Val Loss:3.574|Val Acc:0.3212\n",
      "Epoch:3|Train Loss:2.715|Train Acc:0.374|Val Loss:3.663|Val Acc:0.3158\n",
      "Current lr: 0.01\n",
      "Epoch:4|Train Loss:2.507|Train Acc:0.4024|Val Loss:3.785|Val Acc:0.3186\n",
      "Epoch:5|Train Loss:2.326|Train Acc:0.4318|Val Loss:3.907|Val Acc:0.3104\n",
      "Epoch:6|Train Loss:2.167|Train Acc:0.4622|Val Loss:4.015|Val Acc:0.3077\n",
      "Current lr: 0.005\n",
      "Epoch:7|Train Loss:2.045|Train Acc:0.486|Val Loss:4.12|Val Acc:0.3048\n",
      "Early stop!\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.NLLLoss()                                             # æŒ‡å®šæŸå¤±å‡½æ•°\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARN_RATE)            # æŒ‡å®šä¼˜åŒ–å™¨\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.5)   # å­¦ä¹ ç‡ç¼©å‡\n",
    "\n",
    "SCHED_NUM = 0\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_loss, train_acc = train(model, DEVICE, train_batch, optimizer, criterion, GRAD_CLIP)\n",
    "    valid_loss, valid_acc = evaluate(model, DEVICE, dev_batch, criterion)\n",
    "    if valid_loss < BEST_VALID_LOSS: # å¦‚æœæ˜¯æœ€å¥½çš„æ¨¡å‹å°±ä¿å­˜åˆ°æ–‡ä»¶å¤¹\n",
    "        BEST_VALID_LOSS = valid_loss\n",
    "        torch.save(model, MODEL_PATH.format(EMBEDDING_DIM))\n",
    "        SCHED_NUM = 0\n",
    "    else:\n",
    "        SCHED_NUM += 1\n",
    "        if SCHED_NUM % 3 == 0:\n",
    "            scheduler.step()\n",
    "            print(\"Current lr:\", optimizer.param_groups[0]['lr'])\n",
    "        if SCHED_NUM == 7:\n",
    "            print(\"Early stop!\")\n",
    "            break\n",
    "    print('Epoch:{}|Train Loss:{:.4}|Train Acc:{:.4}|Val Loss:{:.4}|Val Acc:{:.4}'.format(epoch,train_loss,train_acc,valid_loss,valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 3.564551591873169 | Test Acc: 0.31922278920502517 |\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(MODEL_PATH.format(EMBEDDING_DIM))\n",
    "test_loss, test_acc = evaluate(model, DEVICE, test_batch, criterion)\n",
    "print('Test Loss: {0} | Test Acc: {1} |'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 æ‰“å°é”™è¯¯å•è¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç­”åº”é¢„æµ‹é”™è¯¯çš„å•è¯\n",
    "def print_pred_error_words(model,device,data_batch):\n",
    "    model.eval()\n",
    "    error_words = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_batch:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            mask = (y!=PAD_IDX)\n",
    "            fx = model(x)\n",
    "            \n",
    "            pred_idx = fx.argmax(dim=1)\n",
    "            ground_truth_idx = y[mask]\n",
    "            for p, g in zip(pred_idx.tolist(), ground_truth_idx.tolist()):\n",
    "                if p != g:\n",
    "                    error_words.append(\" | \".join([idx2word[g], idx2word[p]]))\n",
    "    return error_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(MODEL_PATH.format(EMBEDDING_DIM))\n",
    "error_words = print_pred_error_words(model, DEVICE, test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "çœŸå®å€¼ | é¢„æµ‹å€¼ | é¢„æµ‹é”™è¯¯æ¬¡æ•°\n",
      "('Bob | He', 137)\n",
      "('She | He', 109)\n",
      "('Sue | He', 89)\n",
      "('and | .', 49)\n",
      "('had | was', 45)\n",
      "('decided | was', 43)\n",
      "('to | .', 42)\n",
      "('her | the', 34)\n",
      "('his | the', 33)\n",
      "('in | .', 30)\n",
      "(', | .', 30)\n",
      "('she | he', 28)\n",
      "('for | .', 28)\n",
      "('His | He', 26)\n",
      "('. | to', 26)\n",
      "('One | He', 25)\n",
      "('. | the', 24)\n",
      "('a | the', 24)\n",
      "('But | He', 21)\n",
      "('Her | He', 21)\n",
      "('The | He', 21)\n",
      "('When | He', 19)\n",
      "('went | was', 19)\n",
      "('They | He', 19)\n",
      "('got | was', 17)\n",
      "('he | Bob', 17)\n",
      "('wanted | was', 17)\n",
      "('! | .', 16)\n",
      "('on | .', 16)\n",
      "('he | to', 16)\n",
      "('at | .', 16)\n",
      "(\"'s | was\", 15)\n",
      "('the | .', 15)\n",
      "('Sue | Bob', 15)\n",
      "('it | the', 14)\n"
     ]
    }
   ],
   "source": [
    "words_counter = Counter(error_words)\n",
    "TopN = 35\n",
    "topn_words = words_counter.most_common(TopN)\n",
    "print(\"çœŸå®å€¼ | é¢„æµ‹å€¼ | é¢„æµ‹é”™è¯¯æ¬¡æ•°\")\n",
    "for w in topn_words:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 æŠ¥å‘Š\n",
    "------\n",
    "### æ¨¡å‹å‚æ•°\n",
    "- BATCH_SIZE = 32       # æ‰¹æ¬¡å¤§å°\n",
    "- EMBEDDING_DIM = 200   # è¯å‘é‡ç»´åº¦\n",
    "- HIDDEN_DIM = 200      # éšå«å±‚\n",
    "- GRAD_CLIP = 5.        # æ¢¯åº¦æˆªæ–­å€¼\n",
    "- EPOCHS = 20           # epoch\n",
    "- LEARN_RATE = 0.01     # åˆå§‹å­¦ä¹ ç‡\n",
    "### å„æŒ‡æ ‡\n",
    "- è®­ç»ƒé›†å‡†ç¡®ç‡ä¸º32%ï¼Œ éªŒè¯é›†å‡†ç¡®ç‡ä¸º32.22%ï¼Œæµ‹è¯•é›†å‡†ç¡®ç‡ä¸º31.92%ã€‚\n",
    "### é”™è¯¯åˆ†æ\n",
    "- è¯·åœ¨ä½ çš„ä»£ç ä¸­æ·»åŠ ä¸€é¡¹åŠŸèƒ½ï¼Œå¯ä»¥å±•ç¤ºå‡ºä½ æ¨¡å‹é¢„æµ‹é”™è¯¯çš„å•è¯ï¼Œå°†æ ‡å‡†ç­”æ¡ˆå•è¯å’Œæ¨¡å‹é¢„æµ‹çš„å•è¯åˆ†åˆ«æ‰“å°å‡ºæ¥ã€‚\n",
    "    - ç­”ï¼šè§ä»£ç \n",
    "- è¯·å†™ä¸‹ä½ çš„æ¨¡å‹æœ€å¸¸è§çš„35ä¸ªé¢„æµ‹é”™è¯¯(æ­£ç¡®ç­”æ¡ˆæ˜¯aï¼Œæ¨¡å‹é¢„æµ‹äº†b)ã€‚\n",
    "```\n",
    "çœŸå®å€¼ | é¢„æµ‹å€¼ | é¢„æµ‹é”™è¯¯æ¬¡æ•°\n",
    "('Bob | He', 137)\n",
    "('She | He', 109)\n",
    "('Sue | He', 89)\n",
    "('and | .', 49)\n",
    "('had | was', 45)\n",
    "('decided | was', 43)\n",
    "('to | .', 42)\n",
    "('her | the', 34)\n",
    "('his | the', 33)\n",
    "('in | .', 30)\n",
    "(', | .', 30)\n",
    "('she | he', 28)\n",
    "('for | .', 28)\n",
    "('His | He', 26)\n",
    "('. | to', 26)\n",
    "('One | He', 25)\n",
    "('. | the', 24)\n",
    "('a | the', 24)\n",
    "('But | He', 21)\n",
    "('Her | He', 21)\n",
    "('The | He', 21)\n",
    "('When | He', 19)\n",
    "('went | was', 19)\n",
    "('They | He', 19)\n",
    "('got | was', 17)\n",
    "('he | Bob', 17)\n",
    "('wanted | was', 17)\n",
    "('! | .', 16)\n",
    "('on | .', 16)\n",
    "('he | to', 16)\n",
    "('at | .', 16)\n",
    "(\"'s | was\", 15)\n",
    "('the | .', 15)\n",
    "('Sue | Bob', 15)\n",
    "('it | the', 14)\n",
    "```\n",
    "- é€šè¿‡è§‚å¯Ÿè¿™äº›å¸¸è§çš„é”™è¯¯ï¼Œå°†é”™è¯¯åˆ†ç±»ã€‚ä½ ä¸éœ€è¦å°†æ¯ä¸ªé”™è¯¯éƒ½åˆ†ç±»ï¼Œä¸è¿‡å»ºè®®åŒå­¦ä»¬èŠ±ç‚¹æ—¶é—´è§‚å¯Ÿè‡ªå·±æ¨¡å‹çš„é”™è¯¯ï¼Œçœ‹çœ‹ä»–ä»¬æ˜¯å¦æœ‰ä¸€å®šçš„ç›¸å…³æ€§ã€‚å¤§å®¶å¯ä»¥å°è¯•ä»ä»¥ä¸‹è§’åº¦æ€è€ƒé”™è¯¯ç±»å‹ï¼š\n",
    "    - ä¸ºä»€ä¹ˆä½ çš„æ¨¡å‹ä¼šé¢„æµ‹å‡ºè¿™ä¸ªå•è¯ï¼Ÿ\n",
    "    - æ¨¡å‹æ€ä¹ˆæ ·æ‰èƒ½åšå¾—æ›´å¥½ï¼Ÿè¿™ä¸ªæ¨¡å‹çŠ¯çš„é”™è¯¯æ˜¯å¾ˆæ¥è¿‘æ­£ç¡®ç­”æ¡ˆçš„å—ï¼Ÿå¦‚æœæ˜¯çš„è¯ï¼Œè¿™ä¸ªé”™è¯¯ç­”æ¡ˆä¸æ­£ç¡®ç­”æ¡ˆæœ‰ä½•ç›¸ä¼¼ä¹‹å¤„ï¼Ÿ\n",
    "    - æŠŠè¿™35ä¸ªé¢„æµ‹é”™è¯¯å½’ç±»æˆä½ å®šä¹‰çš„é”™è¯¯ç±»åˆ«ã€‚è®¨è®ºä¸€ä¸‹ä½ çš„æ¨¡å‹åœ¨å“ªäº›æ–¹é¢åšå¾—æ¯”è¾ƒå¥½ï¼Œå“ªäº›æ–¹é¢åšçš„ä¸å¥½ã€‚\n",
    "    \n",
    " - ç­”ï¼š\n",
    "    - é”™è¯¯åˆ†ç±»ï¼š\n",
    "    - (1)å°†äººåé¢„æµ‹æˆä»£è¯ï¼Œæ¯”å¦‚Bobã€Sueé¢„æµ‹æˆé¾™Heã€‚ä¸ä»…å¦‚æ­¤ï¼Œæ¨¡å‹å°†å¾ˆå¤šè¡Œé¦–çš„è¯éƒ½é¢„æµ‹æˆäº†Heï¼Œè¿™å¯èƒ½æ˜¯è®­ç»ƒçš„æ—¶å€™è¡Œé¦–çš„å•è¯ä¹‹å‰æ²¡æœ‰è¶³å¤Ÿå¤šçš„ä¿¡æ¯ï¼Œå¦‚æœæˆ‘ä»¬å°†å¥å­åŒå‘è¾“å…¥æˆ–è€…åå‘è¾“å…¥è¿›è¡Œè®­ç»ƒå¯èƒ½ä¼šæ”¹å–„è¿™ç§æƒ…å†µã€‚\n",
    "    - (2)ä»£è¯é¢„æµ‹é”™è¯¯ï¼Œåˆ†ä¸æ¸…ç”·å¥³ã€‚Sheé¢„æµ‹æˆHeï¼›sheé¢„æµ‹æˆheç­‰ï¼Œè¿™ç§é”™è¯¯ç›¸å¯¹æ¯”è¾ƒæ¥è¿‘ï¼Œé”™è¯¯ç­”æ¡ˆå’Œæ­£ç¡®ç­”æ¡ˆéƒ½æ˜¯äººç§°ä»£è¯ã€‚\n",
    "    - (3)è¿è¯é¢„æµ‹é”™è¯¯ï¼Œå­˜åœ¨andé¢„æµ‹æˆå¥å·ï¼ŒButé¢„æµ‹æˆHeã€‚\n",
    "    - (4)æ ‡ç‚¹é¢„æµ‹é”™è¯¯ï¼Œé€—å·é¢„æµ‹æˆå¥å·ï¼Œæ„Ÿå¹å·é¢„æµ‹æˆå¥å·ã€‚é”™è¯¯ç­”æ¡ˆå’Œæ­£ç¡®ç­”æ¡ˆç›¸è¿‘ã€‚\n",
    "    - (5)ä»‹è¯é¢„æµ‹é”™è¯¯ã€‚to/in/for/atç­‰éƒ½æœ‰è¢«é¢„æµ‹æˆäº†å¥å·ã€‚é”™è¯¯ç­”æ¡ˆå’Œæ­£ç¡®ç­”æ¡ˆå¹¶ä¸æ¥è¿‘ï¼Œæ¨¡å‹å¯èƒ½å¯¹ä»‹è¯é¢„æµ‹ä¸å¥½ã€‚ä¹‹æ‰€ä»¥å­˜åœ¨è¿™ç§é—®é¢˜ï¼Œå¯èƒ½æ˜¯å› ä¸ºåŸæ¥çš„å¥å­ä¸­to/in/for/at ä¹‹å‰çš„å•è¯å·²ç»èƒ½æ„æˆä¸€ä¸ªå®Œæ•´å¥å­ï¼Œæ‰€ä»¥æ¨¡å‹å¯èƒ½è®¤ä¸ºå‰é¢çš„å•è¯å·²ç»èƒ½æ„æˆå®Œæ•´å¥å­äº†ï¼Œç›´æ¥å°±å°†ä»‹è¯é¢„æµ‹æˆæ ‡ç‚¹äº†ã€‚\n",
    "    - æ¨¡å‹åšå¾—æ¯”è¾ƒå¥½çš„åœ°æ–¹å°±æ˜¯å¥å­çš„ä¸­é—´éƒ¨åˆ†åç»“å°¾éƒ¨åˆ†é¢„æµ‹çš„å‡†ç¡®ç‡é«˜ä¸€äº›ï¼Œè¡Œé¦–é¢„æµ‹çš„å‡†ç¡®ç‡ä½ä¸€äº›ï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºè®­ç»ƒæ˜¯ä»è¡Œé¦–å¼€å§‹è¾“å…¥ï¼Œæ²¡æœ‰å­¦åˆ°ä¹‹å‰çš„ä¿¡æ¯ã€‚è¿›è¡ŒåŒå‘è¾“å…¥è®­ç»ƒå¯èƒ½æ”¹å–„è¿™ç§æƒ…å†µã€‚æ­¤å¤–ï¼Œå¯¹ä»‹è¯é¢„æµ‹ä¸å¥½ï¼Œæ¨¡å‹å¯èƒ½è®¤ä¸ºä»‹è¯ä¹‹å‰çš„å¥å­ç»“æ„å®Œæ•´ï¼Œæ•…ä¼šæŠŠä»‹è¯é¢„æµ‹æˆæ ‡ç‚¹ç¬¦å·ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
